# How to deploy the pipeline and/or serve the model

The pipeline is to make use of SparseML to optimize the model, and then
the KServe InferenceService/ServingRuntime are the one running the
DeepSparse runtime with the model

## Create object data store (MinIO) for the models

Create namespace for the object store if you don't have one

```bash
oc new-project object-datastore
```

Deploy MinIO:

```bash
oc apply -f minio.yaml
```

And create a couple of buckets, one for the pipeline (e.g., named ``mlops``) and one for the models (e.g., named ``models``).

## SparseML

Create pipeline server, pointing to an S3 bucket

Import the pipeline (``sparseml_pipeline.yaml``) into the pipeline server.
This can be generated by running:

```bash
python openshift-ai/pipeline.py
```

### Pipeline Requirements

Cluster storage named ``models-shared``, so that a volume to be shared is created

Data connection, named ``models``, pointing to the S3 bucket to store the resulting model

* NOTE: the cluster storage can have any name, as long as it is the same given later on the pipeline parameters. But the Data connection must be named ``models`` for now as it is hardcoded in the pipeline.

### Create the images needed for the pipeline

Build the container for the sparsification and the evaluation steps:

```bash
podman build -t quay.io/USER/neural-magic:sparseml -f openshift-ai/sparseml_Dockerfile .
podman build -t quay.io/USER/neural-magic:sparseml_eval -f openshift-ai/sparseml_eval_Dockerfile .
podman build -t quay.io/USER/neural-magic:nm_vllm_eval -f openshift-ai/nm_vllm_eval_Dockerfile .
```

And push them to a registry

```bash
podman push quay.io/USER/neural-magic:sparseml
podman push quay.io/USER/neural-magic:sparseml_eval
podman push quay.io/USER/neural-magic:nm_vllm_eval
```

### Run the pipeline

Run the pipeline selecting the model and the options:
- Evaluate or not
- Sparsify or not
- Quantizied or not (note for GPU inferencing, it is not supported to both sparsify and quantized yet)

## DeepSparse

Run the optimized model with DeepSparse

### Create the image needed for the Inference Service

Build the container with:

```bash
podman build -t quay.io/USER/neural-magic:deepsparse -f deepsparse_Dockerfile .
```

And push it to a registry

```bash
podman push quay.io/USER/neural-magic:deepsparse
```

### Option A: Deploy through ServingRuntime

Note DeepSparse require write access to the mounted volume with the model, so doing a workaround so that it gets copied to an extra mount with `ReadOnly` set to `False`.

```bash
oc apply -f openshift-ai/serving_runtime_deepsparse.yaml
```

And them from the OpenShift AI you can deploy a model using it and pointing to the ``models`` DataConnection

### Option B: Deploy InferenceService

Create a secret and a Service Account that points to the S3 endpoint. Modified them as needed.

```bash
oc apply -f openshift-ai/secret.yaml
oc apply -f openshift-ai/sa.yaml

oc apply -f openshift-ai/inference.yaml
```

## nm-vLLM

Run the optimized model with nm-vLLM

### Create the image needed for the ServingRuntime

Build the container with:

```bash
podman build -t quay.io/USER/neural-magic:nm-vllm -f nmvllm_Dockerfile .
```

And push it to a registry

```bash
podman push quay.io/USER/neural-magic:nm-vllm
```

### Deploy through ServingRuntime

Note DeepSparse require write access to the mounted volume with the model, so doing a workaround so that it gets copied to an extra mount with `ReadOnly` set to `False`.

```bash
oc apply -f openshift-ai/serving_runtime_vllm.yaml
oc apply -f openshift-ai/serving_runtime_vllm_marlin.yaml
```

And them from the OpenShift AI you can deploy a model using it and pointing to the ``models`` DataConnection. You can use one or the other depending on running sparsified models or quantized (with ``marlin``) models.


## Testing with Gradio

Run the request.py and access the Gradio server deployed locally at `127.0.0.1:7860`. Update the URL with the one from the deployed runtime (`ksvc` route)

```bash
python openshift-ai/request.py
```
